#!/usr/bin/env python3
# LATTE Phase 3 with RAG support
import json
import requests
import argparse
import os
import time
import google.generativeai as genai
from dotenv import load_dotenv
load_dotenv()

# LLM endpoints
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "mistral"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY", "YOUR_API_KEY")
GEMINI_MODEL = 'gemini-1.5-pro'

# Prompt templates
START_PROMPT_TEMPLATE = """
As a program analyst, I give you snippets of C code generated by decompilation, using
<{function}> as the taint source, and the <{parameter}> parameter marked as the taint label
to extract the taint data flow. Pay attention to the data alias and tainted data operations.
Output in the form of data flows.
<Code to be analyzed>
{code}
"""
MIDDLE_PROMPT_TEMPLATE = """
Continue to analyze function according to the above taint analysis results. Pay attention to
the data alias and tainted data operations.
<Code to be analyzed>
{code}
"""
END_PROMPT_TEMPLATE = """
Based on the above taint analysis results, analyze whether the code has vulnerabilities. If
there is a vulnerability, please explain what kind of vulnerability according to CWE.
"""

# RAG final prompt
RAG_END_PROMPT_TEMPLATE = """
Based on the above taint analysis results, analyze whether the target code has vulnerabilities.

For additional context, here is a similar example of a known CWE-190 vulnerability from my knowledge base:
--- BEGIN KNOWLEDGE BASE CONTEXT ---
{rag_context}
--- END KNOWLEDGE BASE CONTEXT ---

Now, using this example, re-evaluate the target code. Focus specifically on whether an integer overflow is present. Provide your final analysis and the specific CWE if a vulnerability exists.
"""

def query_gemini(messages, retries=3):
    print("--- Sending Prompt to Gemini ---")
    print(messages[-1]['content'])
    
    if GEMINI_API_KEY == "YOUR_API_KEY":
        print("[FATAL] Please set your GOOGLE_API_KEY environment variable.")
        return None

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL)
    
    gemini_messages = []
    for msg in messages:
        if msg['role'] == 'system': continue
        role = 'model' if msg['role'] == 'assistant' else 'user'
        gemini_messages.append({'role': role, 'parts': [msg['content']]})

    for attempt in range(retries):
        try:
            safety_settings = {
                'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
                'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
                'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
            }
            response = model.generate_content(gemini_messages, safety_settings=safety_settings)
            response_text = response.text.strip()
            
            print("--- Gemini Response ---")
            print(response_text + "\n")

            time.sleep(1)
            return response_text
        except Exception as e:
            print(f"[WARN] Gemini API call failed on attempt {attempt + 1}/{retries}: {e}")
            if attempt < retries - 1:
                time.sleep(5 * (attempt + 1))
            else:
                return f"API Error after {retries} attempts: {e}"
    return None


def main():
    parser = argparse.ArgumentParser(description="LATTE Phase 3: Inspect dangerous flows with an LLM.")
    parser.add_argument('--flows-with-code', required=True, help="Path to the flows_with_code.json file.")
    parser.add_argument('--sources', required=True, help="Path to the source classification JSON file.")
    parser.add_argument('--output', required=True, help="Path to save the final vulnerability reports.")
    parser.add_argument('--rag', help="Path to a knowledge base file for RAG mode")
    args = parser.parse_args()

    rag_context = None
    if args.rag:
        try:
            with open(args.rag, 'r') as f:
                rag_context = f.read()
            print(f"[INFO] RAG mode enabled. Loaded knowledge base from: {args.rag}")
        except Exception as e:
            print(f"[WARN] Could not load RAG file '{args.rag}'. Proceeding without RAG. Error: {e}")

    with open(args.flows_with_code, 'r') as f:
        dangerous_flows = json.load(f)
    with open(args.sources, 'r') as f:
        sources_info = {s['function']: s for s in json.load(f)}

    vulnerability_reports = []

    for i, flow in enumerate(dangerous_flows):
        print(f"--- Analyzing Flow #{i+1} / {len(dangerous_flows)} ---")
        
        messages = [{'role': 'system', 'content': 'You are a helpful and concise C security analyst.'}]
        funcs_in_trace = list(dict.fromkeys([step['caller_func'] for step in flow['flow_trace']]))
        
        query_llm = query_gemini

        # Start prompt
        start_func_name = funcs_in_trace[0]
        source_func_name = flow['source_info']['source_function_called']
        source_params = sources_info.get(source_func_name, {}).get('source_result', {}).get('params', '[Unknown]')
        code_for_start_prompt = next((step['code'] for step in flow['flow_trace'] if step['caller_func'] == start_func_name), "")
        start_prompt = START_PROMPT_TEMPLATE.format(function=source_func_name, parameter=str(source_params), code=code_for_start_prompt)
        messages.append({'role': 'user', 'content': start_prompt})
        response = query_llm(messages)
        if response is None: continue
        messages.append({'role': 'assistant', 'content': response})

        # Middle prompts
        if len(funcs_in_trace) > 1:
            for middle_func_name in funcs_in_trace[1:]:
                code_for_middle_prompt = next((step['code'] for step in flow['flow_trace'] if step['caller_func'] == middle_func_name), "")
                middle_prompt = MIDDLE_PROMPT_TEMPLATE.format(code=code_for_middle_prompt)
                messages.append({'role': 'user', 'content': middle_prompt})
                response = query_llm(messages)
                if response is None: break
                messages.append({'role': 'assistant', 'content': response})
        
        # End prompt
        if rag_context:
            # Use RAG prompt when available
            end_prompt = RAG_END_PROMPT_TEMPLATE.format(rag_context=rag_context)
        else:
            # Otherwise use the standard prompt
            end_prompt = END_PROMPT_TEMPLATE
            
        messages.append({'role': 'user', 'content': end_prompt})
        final_judgment = query_llm(messages)
        if final_judgment is None: continue
        messages.append({'role': 'assistant', 'content': final_judgment})

        report = {
            'flow_info': flow,
            'conversation': messages,
            'final_judgment': final_judgment
        }
        vulnerability_reports.append(report)

    with open(args.output, 'w') as f:
        json.dump(vulnerability_reports, f, indent=2)
    print(f"\n[DONE] Saved {len(vulnerability_reports)} vulnerability reports to {args.output}")


if __name__ == "__main__":
    main()
