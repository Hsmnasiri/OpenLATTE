#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
inspect_flows_with_llm.py - Implements LATTE Phase 3: Prompt Sequence Construction.

This script takes the output from the Ghidra analysis (flows_with_code.json)
and converses with a local Ollama LLM to get a final vulnerability judgment.
"""
import json
import requests
import argparse

# --- Ollama Configuration ---
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "mistral" # The model you downloaded with `ollama pull`

# --- Prompt Templates from LATTE Paper (Figure 10) ---

# This prompt starts the conversation for a given flow.
START_PROMPT_TEMPLATE = """
As a program analyst, I give you snippets of C code generated by decompilation, using
<{function}> as the taint source, and the <{parameter}> parameter marked as the taint label
to extract the taint data flow. Pay attention to the data alias and tainted data operations.
Output in the form of data flows.
<Code to be analyzed>
{code}
"""

# This prompt continues the conversation with the next function in the flow.
MIDDLE_PROMPT_TEMPLATE = """
Continue to analyze function according to the above taint analysis results. Pay attention to
the data alias and tainted data operations.
<Code to be analyzed>
{code}
"""

# This prompt asks for the final vulnerability judgment.
END_PROMPT_TEMPLATE = """
Based on the above taint analysis results, analyze whether the code has vulnerabilities. If
there is a vulnerability, please explain what kind of vulnerability according to CWE.
"""

def query_ollama(messages):
    """
    Sends the entire conversation history to the Ollama API and gets the next response.
    """
    print("--- Sending Prompt to Ollama ---")
    # Print only the last user message for brevity
    print(messages[-1]['content'])
    
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={"model": OLLAMA_MODEL, "messages": messages, "stream": False},
            timeout=300 # 5 minute timeout for long responses
        )
        response.raise_for_status()
        response_json = response.json()
        response_text = response_json['message']['content']

        print("--- Ollama Response ---")
        print(response_text + "\n")
        return response_text

    except requests.exceptions.RequestException as e:
        print(f"\n[FATAL] API Error: Could not connect to Ollama at {OLLAMA_API_URL}.")
        print("Please ensure Ollama is running and you have pulled the model with 'ollama pull {}'".format(OLLAMA_MODEL))
        return None

def main():
    parser = argparse.ArgumentParser(description="LATTE Phase 3: Inspect dangerous flows with an LLM.")
    parser.add_argument('--flows-with-code', required=True, help="Path to the flows_with_code.json file.")
    parser.add_argument('--sources', required=True, help="Path to the source classification JSON file.")
    parser.add_argument('--output', required=True, help="Path to save the final vulnerability reports.")
    args = parser.parse_args()

    with open(args.flows_with_code, 'r') as f:
        dangerous_flows = json.load(f)
    with open(args.sources, 'r') as f:
        sources_info = {s['function']: s for s in json.load(f)}

    vulnerability_reports = []

    for i, flow in enumerate(dangerous_flows):
        print(f"--- Analyzing Flow #{i+1} / {len(dangerous_flows)} ---")
        
        # Initialize the conversation for this flow
        messages = [{'role': 'system', 'content': 'You are a helpful and concise C security analyst.'}]
        
        funcs_in_trace = list(dict.fromkeys([step['caller_func'] for step in flow['flow_trace']]))
        
        # 1. Start Prompt
        start_func_name = funcs_in_trace[0]
        source_func_name = flow['source_info']['source_function_called']
        source_params = sources_info.get(source_func_name, {}).get('source_result', {}).get('params', '[Unknown]')
        code_for_start_prompt = next((step['code'] for step in flow['flow_trace'] if step['caller_func'] == start_func_name), "")
        
        start_prompt = START_PROMPT_TEMPLATE.format(function=source_func_name, parameter=str(source_params), code=code_for_start_prompt)
        messages.append({'role': 'user', 'content': start_prompt})
        
        response = query_ollama(messages)
        if response is None: continue # Skip flow if API fails
        messages.append({'role': 'assistant', 'content': response})

        # 2. Middle Prompts (if any)
        if len(funcs_in_trace) > 1:
            for middle_func_name in funcs_in_trace[1:]:
                code_for_middle_prompt = next((step['code'] for step in flow['flow_trace'] if step['caller_func'] == middle_func_name), "")
                middle_prompt = MIDDLE_PROMPT_TEMPLATE.format(code=code_for_middle_prompt)
                messages.append({'role': 'user', 'content': middle_prompt})
                
                response = query_ollama(messages)
                if response is None: break
                messages.append({'role': 'assistant', 'content': response})
        
        # 3. End Prompt
        messages.append({'role': 'user', 'content': END_PROMPT_TEMPLATE})
        final_judgment = query_ollama(messages)
        if final_judgment is None: continue
        messages.append({'role': 'assistant', 'content': final_judgment})

        report = {
            'flow_info': flow,
            'conversation': messages, # Save the full conversation
            'final_judgment': final_judgment
        }
        vulnerability_reports.append(report)

    with open(args.output, 'w') as f:
        json.dump(vulnerability_reports, f, indent=2)
    print(f"\n[DONE] Saved {len(vulnerability_reports)} vulnerability reports to {args.output}")


if __name__ == "__main__":
    main()